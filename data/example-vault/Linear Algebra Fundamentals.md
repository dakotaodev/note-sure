---
tags: math, linear-algebra, ml-foundations
created: 2025-01-10
---

# Linear Algebra Fundamentals

## Core Concepts

### Vectors
- Ordered collections of numbers
- Represent points or directions in space
- Operations: addition, subtraction, dot product, cross product

### Matrices
- 2D arrays of numbers
- Matrix multiplication enables transformations
- Determinant measures matrix properties
- Inverse matrices for solving equations

### Eigenvalues and Eigenvectors
- Eigenvalues: Scaling factors in transformations
- Eigenvectors: Directions that don't change direction
- Used in PCA (Principal Component Analysis)
- Critical for understanding matrix behavior

## Important Operations
- **Transpose** - Flip rows and columns
- **Matrix Multiplication** - Combine transformations
- **Rank** - Number of linearly independent rows/columns
- **Trace** - Sum of diagonal elements

## Applications in ML
- [[Neural Networks]] - Weight matrices and transformations
- [[Vector Embeddings]] - Similarity via dot products
- Dimensionality reduction with eigendecomposition
- Optimization using gradients

## Key Theorems
- Singular Value Decomposition (SVD)
- QR Decomposition
- Cholesky Decomposition
- Spectral Theorem

## Resources
- 3Blue1Brown videos on linear algebra
- Mathematics for Machine Learning (free online book)
- MIT OpenCourseWare lectures

## Related
- [[Machine Learning]]
- [[Neural Networks]]
